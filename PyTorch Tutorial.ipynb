{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with Pytorch: A 60 Minute Blitz\n",
    "blitz: 闪电战\n",
    "\n",
    "Our Goal in this part:\n",
    "\n",
    "+ Understand Pytorch's Tensor library and neural networks at a high level\n",
    "+ Train a small neural network to classify images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's is Pytorch?\n",
    "It is a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "+ A replcement for Numpy to use the power of GPU\n",
    "+ A deep learning search platform that provides maximum flexibility and speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started\n",
    "1. Tensors\n",
    "\n",
    "+ similar as Numpy's ndarrats\n",
    "+ But Tensors can also be used on a GPU to accelerate computing.\n",
    "\n",
    "*Let's take some example now*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_empty= tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n",
      "x_rand= tensor([[0.2634, 0.2545, 0.6092],\n",
      "        [0.7748, 0.0908, 0.8941],\n",
      "        [0.3047, 0.6577, 0.6483],\n",
      "        [0.7937, 0.7073, 0.2769],\n",
      "        [0.3876, 0.6029, 0.7799]])\n",
      "x_LongZeros= tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n",
      "x_data= tensor([2018,    9,   22])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([-0.6584,  1.3335, -0.9023], dtype=torch.float64)\n",
      "torch.Size([9, 9])\n",
      "torch.Size([9, 9])\n",
      "Successful\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_fuction\n",
    "# 适用于从Python2中引入print函数，即便是Python2，Print也可以加()\n",
    "import torch\n",
    "from IPython.display import Latex\n",
    "# Construct a 5x3 matrix, uninitialized:\n",
    "x_empty = torch.empty(5,3)\n",
    "print('x_empty=',x_empty)\n",
    "\n",
    "# Construct a randomly initialized matrix:\n",
    "x_rand = torch.rand(5,3)\n",
    "print('x_rand=',x_rand)\n",
    "\n",
    "# Construct a matrix filled zeros and of dtype long:\n",
    "x_LongZeros = torch.zeros(5, 3, dtype = torch.double)\n",
    "print('x_LongZeros=', x_LongZeros)\n",
    "\n",
    "# Construct a tensor directly from special data:\n",
    "x_data = torch.tensor([2018,9,22])\n",
    "print('x_data=', x_data)\n",
    "\n",
    "# Construct a tensor based on an existing tensor\n",
    "# change Value, Size and dType\n",
    "x_newVST = x_data.new_ones(9, 9, dtype= torch.double)\n",
    "print(x_newVST)\n",
    "# just change Value and dType\n",
    "x_newVS = torch.randn_like(x_data, dtype= torch.double)\n",
    "print(x_newVS)\n",
    "\n",
    "# print Size( Shape)\n",
    "print(x_newVST.size())\n",
    "print(x_newVST.shape)\n",
    "print('Successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Operations\n",
    "There are multiple syntaxes for operations. \n",
    "In the following exampl, we will take a look at the add operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{ADD\\; Operation}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([[0.8121, 0.1098, 0.2129],\n",
      "        [0.4936, 0.7722, 0.4620],\n",
      "        [0.1978, 0.7144, 0.7590],\n",
      "        [0.2558, 0.8495, 0.5203],\n",
      "        [0.5908, 0.9509, 0.6481]])\n",
      "b= tensor([[0.4574, 0.5889, 0.1933],\n",
      "        [0.8983, 0.3399, 0.9876],\n",
      "        [0.8989, 0.1065, 0.3730],\n",
      "        [0.7859, 0.7841, 0.0518],\n",
      "        [0.8448, 0.1018, 0.4269]])\n",
      "c= tensor([[1.2695, 0.6987, 0.4062],\n",
      "        [1.3919, 1.1121, 1.4497],\n",
      "        [1.0967, 0.8209, 1.1320],\n",
      "        [1.0417, 1.6336, 0.5721],\n",
      "        [1.4356, 1.0527, 1.0750]])\n",
      "c= tensor([[1.2695, 0.6987, 0.4062],\n",
      "        [1.3919, 1.1121, 1.4497],\n",
      "        [1.0967, 0.8209, 1.1320],\n",
      "        [1.0417, 1.6336, 0.5721],\n",
      "        [1.4356, 1.0527, 1.0750]])\n",
      "c= tensor([[1.2695, 0.6987, 0.4062],\n",
      "        [1.3919, 1.1121, 1.4497],\n",
      "        [1.0967, 0.8209, 1.1320],\n",
      "        [1.0417, 1.6336, 0.5721],\n",
      "        [1.4356, 1.0527, 1.0750]])\n",
      "add= tensor([[1.2695, 0.6987, 0.4062],\n",
      "        [1.3919, 1.1121, 1.4497],\n",
      "        [1.0967, 0.8209, 1.1320],\n",
      "        [1.0417, 1.6336, 0.5721],\n",
      "        [1.4356, 1.0527, 1.0750]])\n",
      "c[:,0]= tensor([1.2695, 1.3919, 1.0967, 1.0417, 1.4356])\n",
      "c[:,1]= tensor([0.6987, 1.1121, 0.8209, 1.6336, 1.0527])\n",
      "c[:,2]= tensor([0.4062, 1.4497, 1.1320, 0.5721, 1.0750])\n"
     ]
    }
   ],
   "source": [
    "# addition\n",
    "a = torch.rand(5,3)\n",
    "b = torch.rand_like(a)\n",
    "print('a=', a)\n",
    "print('b=', b)\n",
    "\n",
    "# syntax 1 \n",
    "c = a + b;\n",
    "print('c=', c)\n",
    "\n",
    "# syntax 2\n",
    "c1 = torch.add(a,b)\n",
    "print('c=',c1)\n",
    "\n",
    "# syntax 3: providing an output tensor as argument\n",
    "c2 = torch.rand_like(a)\n",
    "torch.add(a, b, out = c2)\n",
    "print('c=',c2)\n",
    "\n",
    "# syntax 4: add a to b\n",
    "b.add_(a)\n",
    "print('add=',b)\n",
    "\n",
    "\"\"\"\n",
    "NOTE: Any operation that mutates(改变) a tensor in-place(就地操作) is post-fixed with an _. \n",
    "For example: x.copy_(y), x.t_(), will change x.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "You can use standard NumPy-like indexing with all bells and whistles! \n",
    "(你可以像使用Numpy库中索引的习惯一样在pytorch中索引)\n",
    "\"\"\"\n",
    "print('c[:,0]=',c[:,0])\n",
    "print('c[:,1]=',c[:,1])\n",
    "print('c[:,2]=',c[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Resizing}$\n",
    "\n",
    "If you want to resize/reshape tensor, you can use torch.view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([[ 0.5465, -1.6677,  1.0034, -0.0125],\n",
      "        [ 0.7528,  0.5030,  1.2933,  0.5500],\n",
      "        [ 1.9160, -2.4091,  0.0134,  0.8555],\n",
      "        [ 1.7463,  0.8464,  1.2658, -0.9102]])\n",
      "x size= torch.Size([4, 4])\n",
      "y size= torch.Size([2, 8])\n",
      "tensor([1.2587]) <built-in method type of Tensor object at 0x000002511A5EFA68>\n",
      "1.2586830854415894 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(-1, 8)\n",
    "print('x=',x)\n",
    "print('x size=',x.size())\n",
    "print('y size=',y.size())\n",
    "\n",
    "\"\"\"\n",
    "(Only) one element tensors can be converted to Python scalars by .item()\n",
    "\"\"\"\n",
    "x = torch.randn(1)\n",
    "print(x, x.type)\n",
    "print(x.item(), type(x.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Operation can be found in http://pytorch.org/docs/torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Numpy Bridge\n",
    "\n",
    "$\\textbf{Converting Torch Tensor to Numpy Array}$\n",
    "\n",
    "Converting a Torch Tensor to a Numpy array and vice versa is a breeze.\n",
    "(注: vice versa 反之亦然; breeze: 轻而易举的事情)\n",
    "\n",
    "The torch Tensor and Numpy array will share their underlying memory locations, and ${\\color{red}{changing\\;one\\;will\\;change\\;the\\;other}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([1., 1., 1., 1., 1.])\n",
      "b= [1. 1. 1. 1. 1.]\n",
      "a= tensor([2., 2., 2., 2., 2.])\n",
      "b= [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print('a=',a)\n",
    "\n",
    "b = a.numpy()\n",
    "print('b=',b)\n",
    "\n",
    "a.add_(1)\n",
    "print('a=',a)\n",
    "print('b=',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Converting Numpy Array to Torch Tensor}$\n",
    "\n",
    "See how changing the np array changed the Torch Tensor automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [1. 1. 1. 1. 1.]\n",
      "b= tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "\"\"\"\n",
    "All the Tensors on the CPU except a CharTensor support converting to NumPy and back.\n",
    "\"\"\"\n",
    "\n",
    "np.add(a,1)\n",
    "print('a=',a)\n",
    "print('b=',b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. CUDA Tensors\n",
    "\n",
    "此部分无法完成，因为此电脑没有可使用的Cuda Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No!\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    print('yes')\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n",
    "else:\n",
    "    print('No!')\n",
    "    \n",
    "# ![jupyter](https://pytorch.org/tutorials/_images/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "Neural networks can be constructed using the torch.nn package.\n",
    "\n",
    "Now that you had a glimpse of 'autograd', 'nn' depends on 'autograd' to define models and differentiate them. An nn.Module contains layers, and a method forward(input) that returns the 'output'.\n",
    "\n",
    "For example, look at this network that classifiers digit images in https://pytorch.org/tutorials/_images/mnist.png, including Input, Convolution, Subsampling, Convolution, Subsampling, Full connection.\n",
    "\n",
    "It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "    + Define the neural network that has some learnable parameters(or weights)\n",
    "    + Iterate over a dataset of inputs\n",
    "    + Process input through the network\n",
    "    + Compute the loss\n",
    "    + Propagate gradients back into the network's parameters\n",
    "    + Update the weights of the network, typically using a simple update rule:\n",
    "\n",
    "$$weight = weight - learning\\_rate * gradient$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define The Network\n",
    "Let's define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel; 6 output channel; 5*5 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        # 16*5*5: input size; 120: output size\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2,2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        # 将x变为全连接。即bathsize * feature_numbers 的二维矩阵\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # 最后一层不需要激活\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        # all dimension except the batch dimension\n",
    "        # 除了第一维之外的维度，例如x是4*5*6的torch张量，则size为torch.Size([5,6])\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        # 返回是30；\n",
    "        # 若输入x:4*5*6*7\n",
    "        # 输出应为30*7=210\n",
    "        # 即所有的特征数量\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "print(net)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have define the *forward* function, and the *backward* function (where gradients are computed) is automatically defined for you using *autograd*. You can use any of the Tensor operation in the *forward* function.\n",
    "\n",
    "The learnable parameters of a model are returned by *net.parameters()*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())   # conv1's weight\n",
    "print(params[1].size())\n",
    "print(params[2].size())\n",
    "print(params[3].size())\n",
    "print(params[4].size())\n",
    "print(params[5].size())\n",
    "print(params[6].size())\n",
    "print(params[7].size())\n",
    "print(params[8].size())\n",
    "print(params[9].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a random $32\\times 32$ imput.\n",
    "\n",
    "Note: Expected input size to this net(LeNet) is $32\\times 32$. To use this net on MNIST dataset, please resize the images from the dataset to $32\\times 32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1585, -0.0513, -0.0055,  0.0208, -0.0938, -0.0586, -0.0479, -0.1212,\n",
      "         -0.1156,  0.0494]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeros the gradient buffers of all parameters and backprops with random gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.rand(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\color{red}{Note}}$\n",
    "\n",
    "*torch.nn* only supports mini-batches. The entire *torch.nn* package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "For example, *nn.conv2d* will take in a 4D Tensor of $nSamples\\times nChannels \\times Height \\times Width$.\n",
    "\n",
    "If you have a single sample, just use *input.unsqueeze(0)* to add a fake batch dimension.(注: squeeze: 挤, 榨, 捏)\n",
    "\n",
    "Before proceeding further, let's recap all the classes you've seen so far.\n",
    "\n",
    "${\\color{blue}{Recap:}}$\n",
    "\n",
    "+ torch.Tensor ---- A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
    "+ nn.Module ---- Neural network module. Convenient way of encapsulating(总结，囊括) parameters, with helpers for moving them to GPU, exporting loading, etc.\n",
    "+ nn.Parameter ---- A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module. \n",
    "+ autograd.Function -- Implements forward and backward definitions of an autograd operation. Every Tensor operation, creates at least a single Function node, that connects to functions that created a Tensor and encodes its history.\n",
    "\n",
    "注\n",
    "+ w.r.t--with regard to; with reference to; with respect to, about的意思，即关于blabla\n",
    "\n",
    "${\\color{blue}{At this point, we covered:}}$\n",
    "\n",
    "+ Defining a neural network\n",
    "+ Processing inputs and calling backward\n",
    "\n",
    "${\\color{blue}{Still Left:}}$\n",
    "\n",
    "+ Computing the loss\n",
    "+ Updating the weights of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "A loss function takes the( output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different $\\color{red}{loss\\; function}$ under teh *nn.package*. A simple loss is: *nn.MSELoss*, which computes the mean-squared error between the input and the target.\n",
    "\n",
    "Take an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5350, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "# a dummy target, for example\n",
    "# 注：dummy: 仿制品。挂名代表，傀儡\n",
    "target = torch.randn(10)\n",
    "# make it the same shape as output\n",
    "target = target.view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "# 注：criterion 规范，标准，准则\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you follow *loss* in the backward direction, using its *.grad_fn* attribute, you will see a graph of computations that looks like this\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "So, when we call *loss.backward()*， the whole graph is differentiated w.r.t the loss, and all Tensors in the graph that has *requires_grad = True* will have their *.grad* Tensor accumulated with the gradient.\n",
    "\n",
    "For illustration, let us follow a few steps backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x000002511A5F48D0>\n",
      "<ThAddmmBackward object at 0x000002511A5F4C50>\n",
      "<ExpandBackward object at 0x000002511A5F48D0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)           #MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop\n",
    "To backpropagate the error all we have to do is to *loss.backward()*. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
    "\n",
    "Now we shall call *loss.backward()*, and have a look at conv1's bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0011, -0.0055, -0.0124,  0.0115,  0.0161, -0.0073])\n"
     ]
    }
   ],
   "source": [
    "# zeros the gradient buffers of all parameters\n",
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have seen how to use loss functions.\n",
    "\n",
    "$\\textbf{Read Later}$:\n",
    "\n",
    "The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is here.\n",
    "\n",
    "$\\textbf{The only thing left to learn is:}$\n",
    "\n",
    "+ Updating the weights of the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update The Weights\n",
    "The simplest update rule used in practice is the Stochastic Gradient Descent(SGD)\n",
    "\n",
    "$$weight = weight - learning\\_rate \\times gradient$$\n",
    "\n",
    "We can implement this using Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data*learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as you use neural networks, you want to use various update rules such as SGD, Neterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: *torch.optim* that implements all these method. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# creat your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()  # zro the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()   # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{NOTE}$\n",
    "\n",
    "Observe how gradient buffers had to be manually set to zero using *optimizer.zero_grad()*. This is because gradients are accumulated as explained in $\\color{red}{Backprop}$ section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training A Classifier\n",
    "This is it. You have seen how to define neural networks, compute loss and make updates to the weights of the network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.215px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
